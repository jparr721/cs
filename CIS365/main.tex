%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
GPU-Accelearated A2C Agents in Halite.io
}


\author{Brendan Caywood$^{1}$, Alexander Fountain$^{2}$, Thomas Bailey${3}$, and Jarred Parr$^{4}$% <-this % stops a space
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Actor Critic Agents provide a robust, reliable way to product highly efficient bots that can self learn as a result of their training.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

The Halite AI competition is commonly seen as a way to build AI best practices, try some new strategies, and also just learn something new. In most cases, people use a minimal amount of the myriad of machine learning based tools available. This is typically done in favor of more simple, and streamlined approaches. The top scoring agents in the leader board are currently mostly hard-coded with complex logic handling situations like collisions, collection, and offensive strategies. Genetic algorithms were fairly common to tune the games parameters and make bots that used their limited senses efficiently. All of the aforementioned strategies were attempted with mediocre results. Whether it was lack of time, ingenuity, or otherwise, our team decided that something else needed to be done. After looking for ways to improve computation times on our genetic algorithm, the idea of reinforcement learning to self tune the entire model was brought up and considered as a viable approach to solve the issues imposed upon the bots.

\section{MATHEMATICAL FOUNDATIONS}

\subsection{Equations}

$$
\alpha + \beta = \chi \eqno{(1)}
$$


\subsection{Some Common Mistakes}
\begin{itemize}


\item Over or under optimizing hyper parameters.
\end{itemize}

\section{CONCLUSIONS}
done

\addtolength{\textheight}{-12cm}
                                  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{thebibliography}{99}

\bibitem{c1} G. O. Young, ÒSynthetic structure of industrial plastics (Book style with paper title and editor),Ó 	in Plastics, 2nd ed. vol. 3, J. Peters, Ed.  New York: McGraw-Hill, 1964, pp. 15Ð64.







\end{thebibliography}




\end{document}
