# Decision Trees (Information Based Learning)
**Idea** - Hierarchical structures implementing a divide and conquer approach
**Cart** - Classification + Recursion Tree
  - Classify instance sorting from root, via testing, down to leaves
    - Each (internal) decision node will specify an attribute value test
    - Each (Leaf) node specifies a classification (numeric)
    - An attribute is associated with each decision node, and based on what it is, we ask a question.

**Linear/Quadratic Discriminant** - We can use a straight or cuved line to fit and separate data into classes when represented in a common setup like a graph
Decision Trees like to partition the data into more dimensions. When you look at a single decision boundry, a decision tree might make smaller boxes in the graph to show how the data partitions better

### The Decision Tree Algorithm
A recursive greedy algorithm

**Question:** What attribute should be tested at the root?
  - The one that best classifies training examples
**Step Two:** Create a descent for each value of an attribute
**Step Three:** Sort training examples down the appropriate branch
**Step Four:** Repeat for each sub tree

### Testing Decision Tree Performance
  - Done on the "goodness" of the split

**Entropy:** A way to measure the "purity" of a sample
  - Ex: If all examples are in the same category, then the entropy is going to be 0
  - Ex: If all examples are evenly mixed, then the entropy = 1

**Formula (for two outcomes, pos and neg)**: 

Entropy(s) = -p(pos) * log2(p(pos)) - p(neg) * log_2 (p(neg))

Multi-CLass Entropy = - sum(i = 1 to n) p(class i) * log_2(p(class i))

**Information Gain:** Measures expected reduction in impurity

**Formula:**

Gain(S, a) = Entropy(S) - sum(values of a) (|S_v| / |S|) * Entropy(S_v)

- values of a = All possible values of attribute a
- S_v = subset of s such that attribute a has a value v