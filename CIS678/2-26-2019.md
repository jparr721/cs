# Feb 26. 2019
## Decision Trees Contd.
- Decision trees are recursive algorithms. They recurse until the classes are pure.
  - Steps for tree generation:
    1. Select the attribute that best classifies the data
    2. Create a descendent for each value of the attribute
    3. Sort the instances down a tree branch

### Purity metrics
**Entropy**
pos = positive outcome
neg = negative outcome
Entropy(S) = -p(pos)log_2(pos) - p(neg)log_2(neg)
  - Bias towards features with many values

**GINI Index**: Expected error if randomly labeled
Fp - False positives
Fn - False negative
p(Fp) + p(Fn)

GINI = 2 * p(pos) * p(neg)
  - Insensitive to distribution

Misclassification error: Error Rate
- %misclassified if labeled all w/ majority label
mis = 1/2 - (p(pos) - 1/2)

### Continuous-Value Data
  - Dynamically partition attribute space > or < a threshold
  1. Sort by attribute value
  2. Examine pairs of adjacent values w/ differing targets
  3. Choose midpoint that maximizes information gain

### Prediction
  - Mean of values of training set instances at the leaf.
  Variance = var(D, t) = sum(i = 1, n) (t_i - t-bar)^2 / n - 1

  D = Dataset leaf
  n = # Instances
  t-bar = Mean target


### Rule Extraction
  - Benefit of rule extraction is that the output is very understandable
  - Feature extraction
    - Ranks the importance of attributes (The attribute that best classifies the set)
 - If-then
    - path: root -> each leaf
    - conjunctive disjunction

### Multivariate Trees
  - Idea: Base decison on multiple attributes
    - Non-Linear (w2\*x2 + w1\*x1 + w0 > 0)
    - Multiple attributes make up this equation


### Random Forest
  - Ensemble Learning
  - Idea: Combine multiple weak learners to form one strong learner
