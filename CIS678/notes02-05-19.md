# Notes 02/05/19

## Bayesian Theory -- A probabilistic model

**Idea:** Random variable drawn from probability distribution

- Use the data itself to define probability
  - Distribution unknown --> Approximate it based on a lot of samples
- Prior Knowledge used to resolve ambiguity
  - Ex. Trying to classify letters, if you have an A and a B and another A that is drawn weirdly to look like a B, you might use prior knowledge to make the determination that it's more likely to be an A since A is used more often



**Characteristics:**

- Look at training examples, they will increase or decrease the probability of our hypothesis
- Prior knowledge + observed data --> Hypothesis Probability
- Accomodates probabilistic predictions
- Hypothesis can be combined

<u>Applies Bayes Theorem:</u>

p(C | X) = p(X | C) p(C) / p(X)

where:

p(C): Prior Probability

p(X | C): Liklihood that an item in class C has observed feature X

p(X): Evidence, the prior probability of X

p(C | X): Posterior Probability, probability that C holds after observing X

**Calculating Posterior**: Posterior = Liklihood * Prior / Evidence

#### Example

p(cancer) = 0.008 // General population

p(+ | cancer) = 0.98 // Correct diagnosis (true positive)

p(+ | no cancer) = 0.03 // False positive

Does the patient have cancer, given a positive test?

p(cancer | +) = p(+ | cancer)p(cancer) / p(+)

--> p(+ | cancer)p(cancer) = 0.98 * 0.008 = 0.0078

--> p(+ | no cancer)p(no cancer) = 0.03 * 0.992 = 0.0298 

Together, these give us our probabability of a positive test

So,

--> p(cancer | +) = 0.98 * 0.008 / 0.0078 + 0.0298 = .2085 --> 21%

### More generally...

p(C<sub>i</sub> | X) = p(X | C<sub>i</sub>) p(C<sub>i</sub>) / sum(j = 1 to K) p(X | C<sub>j</sub>) p(C<sub>j</sub>) --> The k-class problem

## Maximum A Posteriori Learning (MAP)

**Question:** What is the most likely output, given the training data?

**Idea:** Choose C such that for all K we choose the answer that has the highest probability

## Naive Bayes Classifier (MAP)

- Each instance is conjunction of attribute values
  - Ex. Sunny, Warm, Morning --> We go fishing -- This is an instance
  - Rainy, Cold, Evening --> No Fishing -- This is too
  - Attributes would be things like Sunny (Weather), Cold(Temperature), etc...
- Each attribute takes multiple values
  - Per Previous Ex. Sunny/Rainy, Warm/Cold
- **Huge** instance space here (Curse of dimensionality)
- Naive: Attribute values are conditionally independent